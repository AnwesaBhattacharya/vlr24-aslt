# @package _global_

hydra:
  run:
    dir: ${env:SAVE_DIR}/${env:WANDB_NAME}/hydra_outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}

common:
  seed: 48151623
  fp16: False
  wandb_project: ${env:WANDB_PROJECT}
  log_interval: 25

distributed_training:
  heartbeat_timeout: 600

checkpoint:
  save_dir: ${env:SAVE_DIR}/${env:WANDB_NAME}/ckpts/
  save_interval: 5
  keep_last_epochs: 1
  keep_best_checkpoints: 5
  best_checkpoint_metric: sacrebleu
  maximize_best_checkpoint_metric: True
  patience: 15

dataset:
  train_subset: phoenix.mediapipe.train
  valid_subset: phoenix.mediapipe.dev
  num_workers: 4
  batch_size: 32
  max_tokens: 20_000                          # 1600s @ 25fps
  validate_interval: 5
  skip_invalid_size_inputs_valid_test: False

task:
  _name: sign_to_text
  data: ${env:DATA_DIR}
  max_source_positions: 1024                  # ~41s @ 25fps
  max_target_positions: 1024
  normalization: body
  eval_gen_config:
    beam: 5
  eval_bleu: True
  eval_bleu_config:
    sacrebleu_tokenizer: 13a
    sacrebleu_lowercase: False
    sacrebleu_char_level: False
  eval_print_samples: True

model:
  _name: sign2text_transformer
  encoder_embed_dim: 256
  encoder_ffn_embed_dim: 1024
  encoder_attention_heads: 4
  encoder_layers: 6
  decoder_embed_dim: 256
  decoder_ffn_embed_dim: 1024
  decoder_attention_heads: 4
  decoder_layers: 3
  decoder_output_dim: 256
  layernorm_embedding: True
  share_decoder_input_output_embed: True
  dropout: 0.2
  attention_dropout: 0.2
  activation_dropout: 0.2
  activation_fn: relu

criterion:
  _name: label_smoothed_cross_entropy
  label_smoothing: 0.1

optimizer:
  _name: adam
  adam_betas: (0.9, 0.98)
  adam_eps: 1e-08

optimization:
  lr: [1e-03]
  max_update: 40_000
  update_freq: [1]                            # with 1 GPU
  clip_norm: 1

lr_scheduler:
  _name: inverse_sqrt
  warmup_updates: 1000

bpe:
  sentencepiece_model: ${env:DATA_DIR}/phoenix.unigram1000.model